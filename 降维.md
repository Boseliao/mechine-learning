#降维

在很多机器学习问题中，训练集中的每条数据经常伴随着上千、甚至上万个特征。要处理这所有的特征的话，不仅会让训练非常缓慢，还会极大增加搜寻良好解决方案的困难。这个问题就是我们常说的维度灾难。不过值得庆幸的是，在实际问题中，经常可以极大地减少特征的数目，将棘手的问题转变为容易处理的问题。

## 维度灾难

在高维空间中，很多事情的行为都会非常不一样。例如，假设我们在一个单元正方形（1x1正方形）中选择一个随机点，则此点仅有40%的概率与边框的距离小于0.001（也就是说，一个随机点不太可能非常靠近某个维度）。但是在一个10000维的单元超立方体中，这个概率要高于99.999999%。**大部分在高维超平面中的点都非常接近于边界**。

还有一个更麻烦的差异：假设我们在一个单元正方形中随机选取两个点，这两个点的平均距离约为0.52。如果我们在一个3D立方体中随机选择两个点，则平均距离大约为0.66。但是如果我们在1000000维超立方体中随机选择两个点的话，它们的平均距离大约为408.25（约为1000000/6的平方根 ） 。这是一个很反直觉的现象：为什么两个点都在同样的单元超平面中，但是距离可以离的这么远？当然这是由于在高维中有足够多的空间导致了。所以这样导致的结果就是：**高维数据集中的数据点可能会非常稀疏（或离散）**。大多数训练实例可能相互之间离的都非常远，导致预测性能相对于低维数据集来说会更不可靠，因为它们基于的是更大的外推法（extrapolations）。简单地说，训练集的维度越高，过拟合的风险越大。

##降维算法

降维算法主要依赖于以下两个降维方法：投影（projecting）与流形学习（Manifold Learning）。

### 主成成分分析（PCA）

主成成分分析（Principal Component Analysis，PCA）在目前是非常热门的降维算法。首先它找到一个最接近数据的超平面，然后将数据投影到这个平面上。假设有数据集$\{x^1,x^2,...,x^m\},x^i\in R^n$，我的目的是将数据维度减低到$k,k<n$。

数据预处理：

- Set $\mu=\frac{1}{m}\sum_{i=1}^mx^i$
- Replace $x^i$ with $x^i-\mu$
- Set $\sigma_j^2=\frac{1}{m}\sum_{i=1}^m(x^i_j)^2$
- Replace $x^i_j$ with $\frac{x^i_j}{\sigma_j}$

数据x在低维空间的投影表示为：$x^{iT}u$，选择$||u||=1$，为了使数据在低维空间尽量保留原来的分布规律，我们的目标函数设为
$$
\max_{||u||=1}\frac{1}{m}\sum_{i=1}^m|x^{iT}u|^2=u^T\left[\frac{1}{m}\sum_{i=1}^mx^ix^{iT}\right]u=u^T\Sigma u
$$
$\Sigma$是x的谐方差矩阵，利用拉格朗日乘子法，可以得到u的每一列都是$\Sigma$的本征态。

**降维**：选择前k个特征向量$u_1,u_2,...,u_k$，降维后的数据可以表示为，
$$
y^i=\{u^T_1x^i,u^T_2x^i,...,u^T_kx^i\}
$$
当维度n很大时，直接对协方差矩阵求本征态是不方便的。所以，我们一般可以对x做奇异值分解，得到本征向量。

#### 奇异值分解（SVD）

假设有一个矩阵$A\in R^{m\times n}$，可以分解为
$$
A=UDV^T,U\in R^{m\times n},D=Diag(\sigma_1,\sigma_2,...,\sigma_n)\in R^{n\times n},V\in R^{n\times n}
$$
其中：

- U的列：$AA^T$的本征态
- V的列：$A^TA$的本征态

即$x=UDV^T$，选择V中的前k列作为协方差矩阵的特征向量。

## 线性判别分析（LDA）

LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。

### 瑞利商

瑞利商是指这样的函数$R(A,x)$
$$
R(A,x)=\frac{x^HAx^H}{x^Hx}
$$
$x$为非领向量，$A$Hermitan矩阵，即$A^H=A$。瑞利商有一个非常重要的性质，即它的最大值等于矩阵$A$最大的特征值，而最小值等于矩阵$A$的最小的特征值，也就是满足
$$
\lambda_{min}\le\frac{x^HAx}{x^Hx}\le \lambda_{max}
$$
广义瑞利商是指这样的函数$R(A,B,x)$
$$
R(A,B,x)=\frac{x^HAx^H}{x^HBx}
$$
将其通过标准化就可以转化为瑞利商的格式。令$x^{\prime}=B^{1/2}x$，
$$
R(A,B,x)=\frac{x^HAx}{x^HBx}=\frac{x^{\prime H}B^{-1/2}AB^{-1/2}x^{\prime}}{x^{\prime H}x^{\prime}}
$$

###二分类原理

假设数据集$\{x_1,x_2,...,x_m\}$，具有标记$y_j\in\{0,1\}$，$\mu_j$为第j类样本的均值向量，$\Sigma_j$为第j类样本的协方差矩阵，$N_j$为第j类样本的个数。
$$
\mu_j=\frac{1}{N_j}\sum_{i=1}^m\mathbb{I}(y_i=j)x_i,\quad \Sigma_j=\sum_{i=1}^m\mathbb{I}(y_i=j)(x_i-\mu_j)(x_i-\mu_j)^T
$$
由于是两类数据，因此我们只需要将数据投影到一条直线上即可。假设我们的投影直线是向量$\omega$，投影数据的中心以及协方差为$\omega^T\mu_0,\omega^T\mu_1,\omega^T\Sigma_0\omega,\omega^T\Sigma_1\omega$。由于LDA需要让不同类别的数据的类别中心之间的距离尽可能的大，同时我们希望同一种类别数据的投影点尽可能的接近，综上所述，我们的优化目标为：
$$
J=\frac{||\omega^T\mu_0-\omega^T\mu_1||_2^2}{\omega^T(\Sigma_0+\Sigma_1)\omega}
$$
定义类间散度矩阵
$$
S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T
$$
类内散度矩阵
$$
S_{\omega}=\Sigma_0+\Sigma_1=\sum_{j=0}^1\sum_{i=1}^m\mathbb{I}(y_i=j)(x_i-\mu_j)(x_i-\mu_j)^T
$$
优化目标重写为
$$
\omega^*=\arg\max_{\omega}J=\frac{\omega^TS_b\omega}{\omega^TS_{\omega}\omega}
$$
所以，$\omega^*$是矩阵$S_{\omega}^{-1/2}S_bS_{\omega}^{-1/2}$的最大特征值对应的特征向量，假设$\omega$为矩阵$S_{\omega}^{-1}S_b$的特征向量，则$\omega=S_{\omega}^{-1/2}\omega^*$。注意到，对于二分类问题，$S_b\omega$总是平行于$\mu_0-\mu_1$，令$S_b\omega=\lambda(\mu_0-\mu_1)$，得到
$$
S_{\omega}^{-1}S_b\omega=\lambda S_{\omega}^{-1}(\mu_0-\mu_1)=\lambda\omega,\quad \omega=S_{\omega}^{-1}(\mu_0-\mu_1)
$$
所以，
$$
\omega^*=S_{\omega}^{-1/2}(\mu_0-\mu_1)
$$

### 多分类

标记$y_j\in\{0,1,..,k\}$，$m_j$表示第j类的数据个数
$$
S_b=\sum_{j=1}^km_j(\mu_j-\mu)(\mu_j-\mu)^T,\quad S_{\omega}=\sum_{j=0}^1\sum_{i=1}^m\mathbb{I}(y_i=j)(x_i-\mu_j)(x_i-\mu_j)^T
$$
$W\in R^{n\times d}$表示一超平面，J变成一个矩阵，无法作为一个标量函数来优化！也就是说，我们无法直接用二类LDA的优化方法，怎么办呢？一般来说，我们可以用其他的一些替代优化目标来实现。
$$
\max_{W}\frac{tr(W^TS_bW)}{tr(W^TS_{\omega}W)}=\frac{\prod_{i=1}^d\omega_i^TS_b\omega_i}{\prod_{i=1}^d\omega_i^TS_{\omega}\omega_i}
$$
则所求的W是矩阵$S_{\omega}^{-1/2}S_bS_{\omega}^{-1/2}$的前d个本征态所组成的矩阵。

##总结

无监督算法的应用方法

| 数据分布   | 建模概率P(x) | 无法得到概率 |
| ---------- | ------------ | ------------ |
| 子空间     | 因子分析模型 | PCA模型      |
| 成块或聚团 | 混合高斯模型 | K-means模型  |

